{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kpetridis24/vf2-pp/blob/main/benchmark/profile_vf2pp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VF2++ Profile Analysis"
      ],
      "metadata": {
        "id": "0AbsiLUBRbIa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Fqb3Og-JKsjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91952211-c637-4904-e6d9-2562dd3908b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: memory_profiler in /usr/local/lib/python3.7/dist-packages (0.60.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.9.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mThe memory_profiler extension is already loaded. To reload it, use:\n",
            "  %reload_ext memory_profiler\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import collections\n",
        "import random\n",
        "!pip install memory_profiler\n",
        "%load_ext memory_profiler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VF2++ ISO solver"
      ],
      "metadata": {
        "id": "72R8f5wxRMtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_GraphParameters = collections.namedtuple(\n",
        "    \"_GraphParameters\",\n",
        "    [\n",
        "        \"G1\",\n",
        "        \"G2\",\n",
        "        \"G1_labels\",\n",
        "        \"G2_labels\",\n",
        "        \"nodes_of_G1Labels\",\n",
        "        \"nodes_of_G2Labels\",\n",
        "        \"G2_nodes_of_degree\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "_StateParameters = collections.namedtuple(\n",
        "    \"_StateParameters\",\n",
        "    [\"mapping\", \"reverse_mapping\", \"T1\", \"T1_tilde\", \"T2\", \"T2_tilde\"],\n",
        ")\n",
        "\n",
        "\n",
        "def vf2pp_is_isomorphic(G1, G2, node_labels=None, default_label=None):\n",
        "    if vf2pp_all_mappings(G1, G2, node_labels, default_label) is not None:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def vf2pp_all_mappings(G1, G2, node_labels=None, default_label=None):\n",
        "    if G1.number_of_nodes() == 0 or G2.number_of_nodes() == 0:\n",
        "        return False\n",
        "\n",
        "    # Check that both graphs have the same number of nodes and degree sequence\n",
        "    if not nx.faster_could_be_isomorphic(G1, G2):\n",
        "        return False\n",
        "\n",
        "    # Initialize parameters and cache necessary information about degree and labels\n",
        "    graph_params, state_params = _initialize_parameters(\n",
        "        G1, G2, node_labels, default_label\n",
        "    )\n",
        "\n",
        "    # Check if G1 and G2 have the same labels, and that number of nodes per label is equal between the two graphs\n",
        "    if not _precheck_label_properties(graph_params):\n",
        "        return False\n",
        "\n",
        "    # Calculate the optimal node ordering\n",
        "    node_order = _matching_order(graph_params)\n",
        "\n",
        "    # Initialize the stack\n",
        "    stack = []\n",
        "    candidates = iter(_find_candidates(node_order[0], graph_params, state_params))\n",
        "    stack.append((node_order[0], candidates))\n",
        "\n",
        "    mapping = state_params.mapping\n",
        "    reverse_mapping = state_params.reverse_mapping\n",
        "\n",
        "    # Index of the node from the order, currently being examined\n",
        "    matching_node = 1\n",
        "\n",
        "    while stack:\n",
        "        current_node, candidate_nodes = stack[-1]\n",
        "\n",
        "        try:\n",
        "            candidate = next(candidate_nodes)\n",
        "        except StopIteration:\n",
        "            # If no remaining candidates, return to a previous state, and follow another branch\n",
        "            stack.pop()\n",
        "            matching_node -= 1\n",
        "            if stack:\n",
        "                # Pop the previously added u-v pair, and look for a different candidate _v for u\n",
        "                popped_node1, _ = stack[-1]\n",
        "                popped_node2 = mapping[popped_node1]\n",
        "                mapping.pop(popped_node1)\n",
        "                reverse_mapping.pop(popped_node2)\n",
        "                _restore_Tinout(popped_node1, popped_node2, graph_params, state_params)\n",
        "            continue\n",
        "\n",
        "        if _feasibility(current_node, candidate, graph_params, state_params):\n",
        "            # Terminate if mapping is extended to its full\n",
        "            if len(mapping) == G2.number_of_nodes() - 1:\n",
        "                cp_mapping = mapping.copy()\n",
        "                cp_mapping[current_node] = candidate\n",
        "                return cp_mapping\n",
        "\n",
        "            # Feasibility rules pass, so extend the mapping and update the parameters\n",
        "            mapping[current_node] = candidate\n",
        "            reverse_mapping[candidate] = current_node\n",
        "            _update_Tinout(current_node, candidate, graph_params, state_params)\n",
        "            # Append the next node and its candidates to the stack\n",
        "            candidates = iter(\n",
        "                _find_candidates(node_order[matching_node], graph_params, state_params)\n",
        "            )\n",
        "            stack.append((node_order[matching_node], candidates))\n",
        "            matching_node += 1\n",
        "\n",
        "\n",
        "def _precheck_label_properties(graph_params):\n",
        "    G1, G2, G1_labels, G2_labels, nodes_of_G1Labels, nodes_of_G2Labels, _ = graph_params\n",
        "    if any(\n",
        "        label not in nodes_of_G1Labels or len(nodes_of_G1Labels[label]) != len(nodes)\n",
        "        for label, nodes in nodes_of_G2Labels.items()\n",
        "    ):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def _initialize_parameters(G1, G2, node_labels=None, default_label=-1):\n",
        "    G1_labels = dict(G1.nodes(data=node_labels, default=default_label))\n",
        "    G2_labels = dict(G2.nodes(data=node_labels, default=default_label))\n",
        "\n",
        "    graph_params = _GraphParameters(\n",
        "        G1,\n",
        "        G2,\n",
        "        G1_labels,\n",
        "        G2_labels,\n",
        "        nx.utils.groups(G1_labels),\n",
        "        nx.utils.groups(G2_labels),\n",
        "        nx.utils.groups({node: degree for node, degree in G2.degree()}),\n",
        "    )\n",
        "\n",
        "    state_params = _StateParameters(\n",
        "        dict(), dict(), set(), set(G1.nodes()), set(), set(G2.nodes())\n",
        "    )\n",
        "\n",
        "    return graph_params, state_params\n"
      ],
      "metadata": {
        "id": "fQkHUQPLRMWO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matching Node Ordering"
      ],
      "metadata": {
        "id": "BtXKWTdKR34Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "\n",
        "def _matching_order(graph_params):\n",
        "    G1, G2, G1_labels, _, _, nodes_of_G2Labels, _ = graph_params\n",
        "    if not G1 and not G2:\n",
        "        return {}\n",
        "\n",
        "    V1_unordered = set(G1.nodes())\n",
        "    label_rarity = {label: len(nodes) for label, nodes in nodes_of_G2Labels.items()}\n",
        "    used_degrees = {node: 0 for node in G1}\n",
        "    node_order = []\n",
        "\n",
        "    while V1_unordered:\n",
        "        rarest_nodes = _all_argmax(\n",
        "            V1_unordered, key_function=lambda x: -label_rarity[G1_labels[x]]\n",
        "        )\n",
        "        max_node = max(rarest_nodes, key=G1.degree)\n",
        "\n",
        "        for dlevel_nodes in bfs_layers(G1, max_node):\n",
        "            while dlevel_nodes:\n",
        "                max_used_deg_nodes = _all_argmax(\n",
        "                    dlevel_nodes, key_function=lambda x: used_degrees[x]\n",
        "                )\n",
        "                max_deg_nodes = _all_argmax(\n",
        "                    max_used_deg_nodes, key_function=lambda x: G1.degree[x]\n",
        "                )\n",
        "                next_node = min(max_deg_nodes, key=lambda x: label_rarity[G1_labels[x]])\n",
        "\n",
        "                node_order.append(next_node)\n",
        "                for node in G1.neighbors(next_node):\n",
        "                    used_degrees[node] += 1\n",
        "\n",
        "                dlevel_nodes.remove(next_node)\n",
        "                label_rarity[G1_labels[next_node]] -= 1\n",
        "                V1_unordered.discard(next_node)\n",
        "\n",
        "    return node_order\n",
        "\n",
        "\n",
        "def _all_argmax(nodes, key_function):\n",
        "    best_nodes = []\n",
        "    best = -float(\"inf\")\n",
        "    for n in nodes:\n",
        "        if key_function(n) > best:\n",
        "            best = key_function(n)\n",
        "            best_nodes = [n]\n",
        "            continue\n",
        "        if key_function(n) == best:\n",
        "            best_nodes.append(n)\n",
        "\n",
        "    return best_nodes\n"
      ],
      "metadata": {
        "id": "VMxIz7RIR7zS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Candidate Selection"
      ],
      "metadata": {
        "id": "nSNhLp_qSHxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _find_candidates(u, graph_params, state_params):\n",
        "    G1, G2, G1_labels, _, _, nodes_of_G2Labels, G2_nodes_of_degree = graph_params\n",
        "    mapping, reverse_mapping, _, _, _, T2_out = state_params\n",
        "\n",
        "    covered_neighbors = [nbr for nbr in G1[u] if nbr in mapping]\n",
        "    if not covered_neighbors:\n",
        "        candidates = set(nodes_of_G2Labels[G1_labels[u]])\n",
        "        candidates.intersection_update(G2_nodes_of_degree[G1.degree[u]])\n",
        "        candidates.intersection_update(T2_out)\n",
        "        candidates.difference_update(reverse_mapping)\n",
        "        candidates.difference_update(\n",
        "            {\n",
        "                node\n",
        "                for node in candidates\n",
        "                if G1.number_of_edges(u, u) != G2.number_of_edges(node, node)\n",
        "            }\n",
        "        )\n",
        "        return candidates\n",
        "\n",
        "    nbr1 = covered_neighbors[0]\n",
        "    common_nodes = set(G2[mapping[nbr1]])\n",
        "\n",
        "    for nbr1 in covered_neighbors[1:]:\n",
        "        common_nodes.intersection_update(G2[mapping[nbr1]])\n",
        "\n",
        "    common_nodes.difference_update(reverse_mapping)\n",
        "    common_nodes.intersection_update(G2_nodes_of_degree[G1.degree[u]])\n",
        "    common_nodes.intersection_update(nodes_of_G2Labels[G1_labels[u]])\n",
        "    common_nodes.difference_update(\n",
        "        {\n",
        "            node\n",
        "            for node in common_nodes\n",
        "            if G1.number_of_edges(u, u) != G2.number_of_edges(node, node)\n",
        "        }\n",
        "    )\n",
        "    return common_nodes\n"
      ],
      "metadata": {
        "id": "GG-MPQu9SJrD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feasibility Rules "
      ],
      "metadata": {
        "id": "D1VoeD5qSQAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "\n",
        "def _feasibility(node1, node2, graph_params, state_params):\n",
        "    G1 = graph_params.G1\n",
        "\n",
        "    if _cut_PT(node1, node2, graph_params, state_params):\n",
        "        return False\n",
        "\n",
        "    if G1.is_multigraph():\n",
        "        if not _consistent_PT(node1, node2, graph_params, state_params):\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def _cut_PT(u, v, graph_params, state_params):\n",
        "    G1, G2, G1_labels, G2_labels, _, _, _ = graph_params\n",
        "    _, _, T1, T1_tilde, T2, T2_tilde = state_params\n",
        "\n",
        "    u_labels_neighbors = nx.utils.groups({n1: G1_labels[n1] for n1 in G1[u]})\n",
        "    v_labels_neighbors = nx.utils.groups({n2: G2_labels[n2] for n2 in G2[v]})\n",
        "\n",
        "    # if the neighbors of u, do not have the same labels as those of v, NOT feasible.\n",
        "    if set(u_labels_neighbors.keys()) != set(v_labels_neighbors.keys()):\n",
        "        return True\n",
        "\n",
        "    for label, G1_nbh in u_labels_neighbors.items():\n",
        "        G2_nbh = v_labels_neighbors[label]\n",
        "\n",
        "        if isinstance(G1, nx.MultiGraph):\n",
        "            # Check for every neighbor in the neighborhood, if u-nbr1 has same edges as v-nbr2\n",
        "            u_nbrs_edges = sorted(G1.number_of_edges(u, x) for x in G1_nbh)\n",
        "            v_nbrs_edges = sorted(G2.number_of_edges(v, x) for x in G2_nbh)\n",
        "            if any(\n",
        "                u_nbr_edges != v_nbr_edges\n",
        "                for u_nbr_edges, v_nbr_edges in zip(u_nbrs_edges, v_nbrs_edges)\n",
        "            ):\n",
        "                return True\n",
        "\n",
        "        if len(T1.intersection(G1_nbh)) != len(T2.intersection(G2_nbh)):\n",
        "            return True\n",
        "        if len(T1_tilde.intersection(G1_nbh)) != len(T2_tilde.intersection(G2_nbh)):\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def _consistent_PT(u, v, graph_params, state_params):\n",
        "    G1, G2 = graph_params.G1, graph_params.G2\n",
        "    mapping, reverse_mapping = state_params.mapping, state_params.reverse_mapping\n",
        "\n",
        "    for neighbor in G1[u]:\n",
        "        if neighbor in mapping:\n",
        "            if G1.number_of_edges(u, neighbor) != G2.number_of_edges(\n",
        "                v, mapping[neighbor]\n",
        "            ):\n",
        "                return False\n",
        "\n",
        "    for neighbor in G2[v]:\n",
        "        if neighbor in reverse_mapping:\n",
        "            if G1.number_of_edges(u, reverse_mapping[neighbor]) != G2.number_of_edges(\n",
        "                v, neighbor\n",
        "            ):\n",
        "                return False\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "WE24IryPSW3H"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### State Updating/Restoring"
      ],
      "metadata": {
        "id": "OwX9J1sASeok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _update_Tinout(new_node1, new_node2, graph_params, state_params):\n",
        "    G1, G2, _, _, _, _, _ = graph_params\n",
        "    mapping, reverse_mapping, T1, T1_tilde, T2, T2_tilde = state_params\n",
        "\n",
        "    uncovered_neighbors_G1 = {nbr for nbr in G1[new_node1] if nbr not in mapping}\n",
        "    uncovered_neighbors_G2 = {\n",
        "        nbr for nbr in G2[new_node2] if nbr not in reverse_mapping\n",
        "    }\n",
        "\n",
        "    # Add the uncovered neighbors of node1 and node2 in T1 and T2 respectively\n",
        "    T1.update(uncovered_neighbors_G1)\n",
        "    T2.update(uncovered_neighbors_G2)\n",
        "    T1.discard(new_node1)\n",
        "    T2.discard(new_node2)\n",
        "\n",
        "    T1_tilde.difference_update(uncovered_neighbors_G1)\n",
        "    T2_tilde.difference_update(uncovered_neighbors_G2)\n",
        "    T1_tilde.discard(new_node1)\n",
        "    T2_tilde.discard(new_node2)\n",
        "\n",
        "\n",
        "def _restore_Tinout(popped_node1, popped_node2, graph_params, state_params):\n",
        "    # If the node we want to remove from the mapping, has at least one covered neighbor, add it to T1.\n",
        "    G1, G2, _, _, _, _, _ = graph_params\n",
        "    mapping, reverse_mapping, T1, T1_out, T2, T2_out = state_params\n",
        "\n",
        "    is_added = False\n",
        "    for nbr in G1[popped_node1]:\n",
        "        if nbr in mapping:\n",
        "            T1.add(\n",
        "                popped_node1\n",
        "            )  # if a neighbor of the excluded node1 is in the mapping, keep node1 in T1\n",
        "            is_added = True\n",
        "        else:  # check if its neighbor has another connection with a covered node. If not, only then exclude it from T1\n",
        "            if any(nbr2 in mapping for nbr2 in G1[nbr]):\n",
        "                continue\n",
        "            T1.discard(nbr)\n",
        "            T1_out.add(nbr)\n",
        "\n",
        "    # Case where the node is not present in neither the mapping nor T1. By deffinition it should belong to T1_out\n",
        "    if not is_added:\n",
        "        T1_out.add(popped_node1)\n",
        "\n",
        "    is_added = False\n",
        "    for nbr in G2[popped_node2]:\n",
        "        if nbr in reverse_mapping:\n",
        "            T2.add(popped_node2)\n",
        "            is_added = True\n",
        "        else:\n",
        "            if any(nbr2 in reverse_mapping for nbr2 in G2[nbr]):\n",
        "                continue\n",
        "            T2.discard(nbr)\n",
        "            T2_out.add(nbr)\n",
        "\n",
        "    if not is_added:\n",
        "        T2_out.add(popped_node2)\n"
      ],
      "metadata": {
        "id": "B4wvwY3kSiEN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Profile Analysis"
      ],
      "metadata": {
        "id": "sQ1e9uLVS-HX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization"
      ],
      "metadata": {
        "id": "bVOLbmWhT7HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Label values\n",
        "colors = [\n",
        "    \"white\",\n",
        "    \"black\",\n",
        "    \"green\",\n",
        "    \"purple\",\n",
        "    \"orange\",\n",
        "    \"red\",\n",
        "    \"blue\",\n",
        "    \"pink\",\n",
        "    \"yellow\",\n",
        "    \"none\",\n",
        "]\n",
        "\n",
        "# Create Graphs\n",
        "G1 = nx.gnp_random_graph(450, 0.6, 42)\n",
        "G2 = nx.gnp_random_graph(450, 0.6, 42)\n",
        "\n",
        "# Assign Labels \n",
        "nx.set_node_attributes(G1, dict(zip(G1, itertools.cycle(colors))), \"label\")\n",
        "nx.set_node_attributes(G2, dict(zip(G1, itertools.cycle(colors))), \"label\")"
      ],
      "metadata": {
        "id": "mpKGoZjfTLcM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bfs_layers(G, sources):\n",
        "    if sources in G:\n",
        "        sources = [sources]\n",
        "\n",
        "    current_layer = list(sources)\n",
        "    visited = set(sources)\n",
        "\n",
        "    for source in current_layer:\n",
        "        if source not in G:\n",
        "            raise nx.NetworkXError(f\"The node {source} is not in the graph.\")\n",
        "\n",
        "    # this is basically BFS, except that the current layer only stores the nodes at\n",
        "    # same distance from sources at each iteration\n",
        "    while current_layer:\n",
        "        yield current_layer\n",
        "        next_layer = list()\n",
        "        for node in current_layer:\n",
        "            for child in G[node]:\n",
        "                if child not in visited:\n",
        "                    visited.add(child)\n",
        "                    next_layer.append(child)\n",
        "        current_layer = next_layer"
      ],
      "metadata": {
        "id": "IzYEXOkaoIFM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CPU utilization"
      ],
      "metadata": {
        "id": "0eTtmPCRoGsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit vf2pp_is_isomorphic(G1, G2, node_labels=None)"
      ],
      "metadata": {
        "id": "QRCmQm3pUCVI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf6758b6-b691-40c7-9d25-333dffa2f00d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "442 ms ± 3.84 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit nx.is_isomorphic(G1, G2)"
      ],
      "metadata": {
        "id": "X8jGkjqsZY92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Memory consumption"
      ],
      "metadata": {
        "id": "eFnuH8xUoPGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%memit vf2pp_is_isomorphic(G1, G2, node_labels=None)"
      ],
      "metadata": {
        "id": "kFv6kAYdnXQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f6bd086-847c-4e2b-d8b5-6d185d7eda39"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "peak memory: 169.55 MiB, increment: 0.12 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%memit nx.is_isomorphic(G1, G2)"
      ],
      "metadata": {
        "id": "PhxrUWelEd9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4df6115-d79a-40e9-8e87-b2a51605a911"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "peak memory: 169.91 MiB, increment: 0.36 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stats"
      ],
      "metadata": {
        "id": "T_BT6ywxFMLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%prun vf2pp_is_isomorphic(G1, G2, node_labels=None)"
      ],
      "metadata": {
        "id": "mbfctx8wFPjR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7176176d-a1a1-4698-9225-e2ede7d38c7d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        }
      ]
    }
  ]
}